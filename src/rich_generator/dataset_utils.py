"""
Dataset utilities for synthetic RICH Cherenkov rings.

This module provides a collection of functions to load and save datasets
generated by :class:`rich_generator.generator.SCGen` and to produce
training data for machine-learning models such as YOLO.

The functions in this module include:

- :func:`calculate_cherenkov_angle` and :func:`calculate_cherenkov_radius`:
  simple helpers to compute the Cherenkov angle and radius.
- :func:`read_dataset` and :func:`read_dataset_metadata`: load datasets
  stored by :meth:`rich_generator.generator.SCGen.save_dataset`.
- :func:`generate_MYOLO_dataset`: produce a new dataset with uniformly
  distributed momenta and centre positions by adding signal rings to
  background events.
- :func:`generate_MYOLO_dataset_folder`: convert an HDF5 dataset into
  a folder of images with accompanying train/val/test splits.

These utilities are intended for offline data preparation.  For
interactive visualisation of events see :func:`plot_event` in the
original project.

"""

from __future__ import annotations

import os
import json
from typing import Any, Dict, Iterable, Tuple, Optional

import numpy as np
import h5py
from tqdm.auto import tqdm
from scipy.stats import gaussian_kde

__all__ = [
    "calculate_cherenkov_angle",
    "calculate_cherenkov_radius",
    "read_dataset",
    "read_dataset_metadata",
    "generate_MYOLO_dataset",
    "generate_MYOLO_dataset_folder",
]


def calculate_cherenkov_angle(momentum: float, mass: float, refractive_index: float) -> float:
    """Return the Cherenkov angle in radians for a particle.

    Parameters
    ----------
    momentum : float
        Particle momentum in GeV/c.
    mass : float
        Particle mass in GeV/c².
    refractive_index : float
        Index of refraction of the radiator medium.

    Returns
    -------
    float
        The Cherenkov emission angle.  If the argument of the
        arccosine exceeds unity (i.e. below threshold) the angle is
        zero.
    """
    beta = momentum / np.sqrt(momentum ** 2 + mass ** 2)
    arg = 1.0 / (refractive_index * beta)
    if arg > 1.0:
        # below threshold - no Cherenkov radiation
        return 0.0
    theta_c = np.arccos(arg)
    return theta_c


def calculate_cherenkov_radius(theta_c: float, refractive_index: float, MAX_RADIUS: float = 100.0) -> float:
    """Convert a Cherenkov angle to a projected ring radius.

    The maximum radius ``MAX_RADIUS`` corresponds to the maximum
    Cherenkov angle for the given medium (i.e. when the particle is
    ultra-relativistic).  The formula rescales smaller angles
    accordingly.
    """
    R_max = float(MAX_RADIUS)
    theta_c_max = np.arccos(1.0 / refractive_index)
    if theta_c_max == 0.0:
        return 0.0
    R = np.tan(theta_c) * R_max / np.tan(theta_c_max)
    return R


def _read_rings(rings_ds) -> list[np.ndarray]:
    """Internal helper to load variable-length ring datasets from HDF5.

    Each ring is stored as a variable-length dataset with dtype
    ``(x,y)``.  This function reconstructs a list of ``(N_i,2)`` arrays.
    """
    rings: list[np.ndarray] = []
    for idx in range(len(rings_ds)):
        try:
            r = rings_ds[idx]
            if r is not None and hasattr(r, 'dtype') and 'x' in r.dtype.names and 'y' in r.dtype.names:
                rings.append(np.stack([r['x'], r['y']], axis=1))
            else:
                rings.append(np.empty((0, 2), dtype=np.float32))
        except Exception:
            rings.append(np.empty((0, 2), dtype=np.float32))
    return rings


def read_dataset(data_file: str, events: Optional[Iterable[int]] = None, silent: bool = False) -> list[Dict[str, Any]]:
    """Load an entire dataset or selected events from an HDF5 file.

    Parameters
    ----------
    data_file : str
        Path to the ``.h5`` file created by :meth:`SCGen.save_dataset`.
    events : iterable of int, optional
        Indices of events to read.  If ``None`` all events are loaded.
    silent : bool, optional
        Suppress progress output when ``True``.

    Returns
    -------
    list of dict
        The dataset in the same format returned by
        :meth:`SCGen.generate_dataset`.
    """
    dataset: list[Dict[str, Any]] = []
    with h5py.File(data_file, 'r') as f:
        if events is None:
            N_events = int(f.attrs['num_events'])
            events = range(N_events)
        else:
            # ensure events is a concrete list for iteration
            events = list(events)
        if not silent:
            print(f"Reading dataset from {data_file} with {len(events)} events...")
        for i in tqdm(events, disable=silent, desc="Reading dataset"):
            grp = f[f'event_{i}']
            event = {
                'centers': np.array(grp['centers']),
                'momenta': np.array(grp['momenta']),
                'particle_types': np.array(grp['particle_types']),
                'rings': _read_rings(grp['rings']),
                'tracked_rings': np.array(grp['tracked_rings'])
            }
            dataset.append(event)
    return dataset


def read_dataset_metadata(data_file: str) -> Dict[str, Any]:
    """Return metadata stored in the header of a dataset file.

    The metadata includes the refractive index, detector size, radial
    noise, initial photon count, maximum Cherenkov radius, particle
    types and masses.  See :meth:`SCGen.save_dataset` for details.
    """
    with h5py.File(data_file, 'r') as f:
        metadata = {
            'refractive_index': float(f.attrs['refractive_index']),
            'detector_size': np.asarray(f.attrs['detector_size'], dtype=np.float32),
            'radial_noise': np.asarray(f.attrs['radial_noise'], dtype=np.float32),
            'N_init': int(f.attrs['N_init']),
            'max_cherenkov_radius': float(f.attrs['max_cherenkov_radius']),
            'particle_types': list(np.asarray(f.attrs['particle_types'], dtype=np.int32)),
            'masses': json.loads(f.attrs['masses']),
            'num_events': int(f.attrs['num_events'])
        }
    return metadata


class TruncatedKDE(gaussian_kde):
    """A KDE that supports per-dimension truncation of the sampling region.

    This class subclasses :class:`scipy.stats.gaussian_kde` and adds
    support for rejecting samples that fall outside user-supplied
    intervals for each dimension.

    Parameters
    ----------
    dataset : (d, N) array_like
        Training samples as required by :class:`gaussian_kde`.
    bw_method : {None, 'scott', 'silverman', scalar} or callable, optional
        Bandwidth selection passed to the parent class.
    truncate_range : None | (a, b) | sequence[(a_i, b_i)]
        Per-dimension bounds.  ``None`` disables truncation, a single
        ``(a,b)`` tuple applies to 1D data, and a list of length ``d``
        applies to higher dimensions.
    """

    def __init__(self, dataset, bw_method=None, truncate_range=None):
        dataset = np.atleast_2d(dataset)
        super().__init__(dataset, bw_method=bw_method)
        # normalise truncate_range to a list of tuples
        if truncate_range is None:
            self.truncate_range = None
        else:
            if self.d == 1 and len(truncate_range) == 2 and not isinstance(truncate_range[0], (list, tuple)):
                truncate_range = [truncate_range]
            if len(truncate_range) != self.d:
                raise ValueError(f"truncate_range must have {self.d} intervals (got {len(truncate_range)})")
            self.truncate_range = [tuple(r) for r in truncate_range]

    def __call__(self, x, *args, **kwargs):
        result = super().__call__(x, *args, **kwargs)
        if self.truncate_range is not None:
            x = np.atleast_2d(x)
            mask = np.ones(x.shape[1], dtype=bool)
            for dim, (a, b) in enumerate(self.truncate_range):
                mask &= (x[dim] >= a) & (x[dim] <= b)
            result[~mask] = 0.0
        return result

    def resample(self, size=1, *, max_iter: int = 10_000):
        """Draw samples respecting the truncation bounds.

        Parameters
        ----------
        size : int
            Number of samples to draw.
        max_iter : int, optional
            Maximum iterations for the rejection loop.  A
            ``RuntimeError`` is raised if this limit is exceeded.
        """
        if self.truncate_range is None:
            return super().resample(size)
        accepted = []
        remaining = size
        iterations = 0
        while remaining > 0:
            iterations += 1
            if iterations > max_iter:
                raise RuntimeError(
                    "Maximum iterations exceeded while rejection-sampling. "
                    "The truncation region may be extremely small."
                )
            # oversample by a factor of 2 to reduce the expected number of loops
            batch = super().resample(int(remaining * 2))
            mask = np.ones(batch.shape[1], dtype=bool)
            for dim, (a, b) in enumerate(self.truncate_range):
                mask &= (batch[dim] >= a) & (batch[dim] <= b)
            if np.any(mask):
                accepted.append(batch[:, mask])
                remaining = size - sum(arr.shape[1] for arr in accepted)
        samples = np.hstack(accepted)[:, :size]
        return samples

def save_kde(kde, filename):
    """Save a gaussian_kde or TruncatedKDE instance to an .npz file.

    Parameters
    ----------
    kde : gaussian_kde or TruncatedKDE
        The KDE object to save.
    filename : str
        Path (including “.npz”) where the object will be written.
    """
    # Determine if this is a TruncatedKDE (has attribute `truncate_range`)
    is_truncated = hasattr(kde, "truncate_range") and (kde.truncate_range is not None)

    # Extract the original dataset (shape: (d, N))
    dataset = kde.dataset

    # Extract a numeric bandwidth factor so that reconstruction is identical.
    bw = float(kde.covariance_factor())

    # Prepare the “is_truncated” flag as 0/1
    flag = np.array([1], dtype=np.uint8) if is_truncated else np.array([0], dtype=np.uint8)

    if is_truncated:
        # Convert truncate_range (list of tuples) into a 2D array of shape (d, 2)
        tr = np.asarray(kde.truncate_range, dtype=float)  # shape = (d, 2)
        np.savez(
            filename,
            dataset=dataset,
            bw_method=bw,
            is_truncated=flag,
            truncate_range=tr
        )
    else:
        # No truncation: still save an empty placeholder for truncate_range
        tr = np.empty((0, 2), dtype=float)
        np.savez(
            filename,
            dataset=dataset,
            bw_method=bw,
            is_truncated=flag,
            truncate_range=tr
        )


def load_kde(filename: str):
    """Load a KDE or truncated KDE from a ``.npz`` file.

    Used to load KDEs saved with the :func:`save_kde` function.
    """
    data = np.load(filename, allow_pickle=False)
    dataset = data["dataset"]  # shape = (d, N)
    bw = float(data["bw_method"])  # scalar bandwidth factor
    is_trunc = bool(int(data["is_truncated"][0]))
    if is_trunc:
        tr_array = data["truncate_range"]
        truncate_range = [tuple(row) for row in tr_array]
        kde = TruncatedKDE(dataset, bw_method=bw, truncate_range=truncate_range)
    else:
        kde = gaussian_kde(dataset, bw_method=bw)
    return kde


def _compare_metadata(meta1: Dict[str, Any], meta2: Dict[str, Any]) -> Tuple[bool, list[str]]:
    """Check that two metadata dictionaries match (ignoring ``num_events``).

    Returns a tuple ``(match, mismatches)`` where ``match`` is ``True``
    if the metadata agree and ``mismatches`` contains a list of human
    readable descriptions of any differences.
    """
    mismatches: list[str] = []
    keys1 = set(meta1.keys()) - {'num_events'}
    keys2 = set(meta2.keys()) - {'num_events'}
    if keys1 != keys2:
        missing1 = keys2 - keys1
        missing2 = keys1 - keys2
        if missing1:
            mismatches.append(f"Missing in first metadata: {missing1}")
        if missing2:
            mismatches.append(f"Missing in second metadata: {missing2}")
        return False, mismatches
    # compare values ignoring order for arrays/lists
    def _to_hashable(x):
        if isinstance(x, np.ndarray):
            return tuple(_to_hashable(i) for i in x.tolist())
        elif isinstance(x, list):
            return tuple(_to_hashable(i) for i in x)
        elif isinstance(x, dict):
            return tuple(sorted((k, _to_hashable(v)) for k, v in x.items()))
        else:
            return x
    for k in keys1:
        v1 = meta1[k]
        v2 = meta2[k]
        if isinstance(v1, np.ndarray) and isinstance(v2, np.ndarray):
            if set(_to_hashable(v1)) != set(_to_hashable(v2)):
                mismatches.append(f"Key '{k}': arrays differ (as sets): {v1} vs {v2}")
        elif isinstance(v1, list) and isinstance(v2, list):
            if set(_to_hashable(v1)) != set(_to_hashable(v2)):
                mismatches.append(f"Key '{k}': lists differ (as sets): {v1} vs {v2}")
        else:
            if v1 != v2:
                mismatches.append(f"Key '{k}': {v1} != {v2}")
    return len(mismatches) == 0, mismatches


def generate_MYOLO_dataset(
    momenta_range: Tuple[float, float],
    output_file: Optional[str] = None,
    particles: Iterable[int] = (321, 211, 2212, 11, 13),
    num_added_ring_per_event: int = 5,
    **kwargs
) -> list[Dict[str, Any]] | None:
    """Generate a dataset with uniformly distributed momenta and centres.

    The function produces background events using :class:`SCGen`, then
    inserts additional rings with uniformly sampled momenta and centre
    positions.  If ``output_file`` is provided the resulting dataset
    will be written to disk; otherwise it is returned as a Python
    object.

    Parameters
    ----------
    momenta_range : tuple
        Minimum and maximum particle momentum (GeV/c) for the added rings.
    output_file : str, optional
        Path to the output ``.h5`` file.  If ``None`` the dataset is
        returned instead of being saved.
    particles : iterable of int, optional
        PDG codes of particles to include.  Defaults to K⁺, π⁺, p, e⁻,
        μ⁻.
    num_added_ring_per_event : int, optional
        Number of rings to add to each event.
    **kwargs : dict, optional
        Additional parameters forwarded to :class:`SCGen` and
        :meth:`SCGen.generate_dataset`.

    Returns
    -------
    list of dict or None
        The generated dataset if ``output_file`` is ``None``, otherwise
        ``None``, since the dataset is saved to disk.
    """
    # defer import to avoid circular dependency
    from .generator import SCGen

    # determine momenta distributions
    if 'momenta_log_distributions' not in kwargs:
        momenta_log_distributions: Dict[int, Any] = {}
        this_dir = os.path.dirname(os.path.abspath(__file__))
        for ptype in particles:
            kde_path = os.path.join(this_dir, '..', '..', 'distributions', 'log_momenta_kdes', f'{ptype}-kde.npz')
            if os.path.exists(kde_path):
                momenta_log_distributions[ptype] = load_kde(kde_path)
            else:
                raise FileNotFoundError(f"KDE file not found for particle {ptype}: {kde_path}")
    else:
        momenta_log_distributions = kwargs['momenta_log_distributions']

    # build SCGen parameters with defaults
    scgen_params = {
        'particle_types': kwargs.get('particle_types', list(particles)),
        'refractive_index': kwargs.get('refractive_index', 1.0014),
        'detector_size': kwargs.get('detector_size', ((-600, 600), (-600, 600))),
        'momenta_log_distributions': momenta_log_distributions,
        'centers_distribution': kwargs.get('centers_distribution', None),
        'radial_noise': kwargs.get('radial_noise', (0, 1.5)),
        'N_init': kwargs.get('N_init', 60),
        'max_radius': kwargs.get('max_radius', 100.0),
        'masses': kwargs.get('masses', None),
    }
    # default centre distribution uses a KDE if not provided
    if scgen_params['centers_distribution'] is None:
        this_dir = os.path.dirname(os.path.abspath(__file__))
        centres_kde_path = os.path.join(this_dir, '..', '..', 'distributions', 'centers_R1-kde.npz')
        if os.path.exists(centres_kde_path):
            scgen_params['centers_distribution'] = load_kde(centres_kde_path)
        else:
            raise FileNotFoundError(f"Centre KDE file not found: {centres_kde_path}")

    # parameters for event generation
    generate_dataset_params = {
        'num_events': kwargs.get('num_events', 20_000),
        'num_particles_per_event': kwargs.get('num_particles_per_event', (170, 180)),
        'parallel': kwargs.get('parallel', True),
        'progress_bar': kwargs.get('progress_bar', True),
    }

    # generate background events
    scgen = SCGen(**scgen_params)
    scgen.generate_dataset(**generate_dataset_params)
    scgen.prune_centers(0)  # treat all existing rings as background

    # insert uniformly sampled rings
    total_num_rings = generate_dataset_params['num_events'] * num_added_ring_per_event
    ptypes = np.random.choice(list(particles), size=total_num_rings)
    momenta = np.random.uniform(momenta_range[0], momenta_range[1], size=total_num_rings)
    # uniformly sample centres within the detector boundaries
    detector_size = scgen_params['detector_size']
    centres = np.random.uniform(
        [detector_size[0][0], detector_size[1][0]],
        [detector_size[0][1], detector_size[1][1]],
        size=(total_num_rings, 2)
    )
    # insert rings into each event
    for i in range(generate_dataset_params['num_events']):
        start = i * num_added_ring_per_event
        end = (i + 1) * num_added_ring_per_event
        scgen.insert_rings(
            event=i,
            centers=centres[start:end],
            momenta=momenta[start:end],
            particle_types=ptypes[start:end]
        )
    if output_file is None:
        return scgen.dataset
    else:
        scgen.save_dataset(output_file)
        return None


def generate_MYOLO_dataset_folder(
    data_file: str,
    base_dir: str,
    image_size: int = 128,
    max_photons: float = 1.0,
    train_val_test_split: Tuple[float, float, float] = (0.8, 0.1, 0.1),
    train_data_file: Optional[str] = None,
    annular_mask: bool = True,
    polar_transform: bool = True,
    seed: int = 42,
    as_png: bool = False,
    debug_hilight: bool = False,
    debug_normalize: bool = False,
    images_per_folder: Optional[int] = 1_000,
    *,
    stretch_radii: bool = False,
) -> None:
    """Generate a MYOLO dataset folder from an HDF5 dataset.

    This function converts an HDF5 dataset created by
    :class:`SCGen` into a set of images centred on each ring and
    corresponding text files enumerating the train/validation/test
    splits. The images are optionally annulus-masked and/or polar
    transformed. When ``stretch_radii`` is ``True`` the polar
    transform stretches the ring radii across the full image height.

    The dataset is split into train, validation, and test sets based on the provided split ratios.
    The folder structure will be:

    MYOLO_dataset/
        ├── images/
        │   ├── image_0.jpg
        │   ├── image_1.jpg
        │   └── ...
        ├── train.txt
        ├── val.txt
        ├── test.txt
        └── particles.txt  (List of particles in the dataset)

    Parameters
    ----------
    data_file : str
        Path to the ``.h5`` dataset file.
    base_dir : str
        Directory in which to create the dataset.  Subdirectories
        ``images``, ``train.txt``, ``val.txt`` and ``test.txt`` will be
        created.
    image_size : int, optional
        Output image height and width in pixels.  Defaults to 128.
    max_photons : float, optional
        Maximum expected number of photons per square millimetre; used
        to normalise the hit counts.
    train_val_test_split : tuple of float, optional
        Fractional split of the rings into training, validation and
        testing sets.  Must sum to 1.0.
    train_data_file : str, optional
        Optional HDF5 file containing additional training data.  If
        provided it is prepended to the dataset and the training
        fraction is interpreted relative to the remaining rings.
    annular_mask : bool, optional
        Apply an annulus mask around each ring prior to the polar
        transform.
    polar_transform : bool, optional
        Convert hits into polar coordinates around each centre.  If
        ``False`` the images are rendered in Cartesian coordinates.
    seed : int, optional
        Seed for the random shuffling of rings into train/val/test.
    as_png : bool, optional
        Save images as ``.png`` files when ``True``; otherwise ``.npy``
        arrays are written.
    debug_hilight : bool, optional
        When ``True`` produce RGB images highlighting the main ring.
    debug_normalize : bool, optional
        When ``True`` normalise images to [0,1] for debugging.
    images_per_folder : int or None, optional
        Limit the number of images per subdirectory to avoid slow
        filesystem performance.  If ``None`` no sharding is performed.
    stretch_radii : bool, keyword-only, optional
        Use the polar mapping that stretches radii across the
        entire image height.  Defaults to ``False`` which uses the
        corrected mapping with a dark band outside the annulus.
        If ``True`` after the application of the annular mask the radii
        are stretched so that the outer radius of the annulus maps to the
        top of the image and the inner radius maps to the bottom.
    """
    # validate input
    if not isinstance(train_val_test_split, (tuple, list)) or len(train_val_test_split) != 3:
        raise ValueError("train_val_test_split must have three elements (train, val, test).")
    if not np.isclose(sum(train_val_test_split), 1.0):
        raise ValueError("train_val_test_split must sum to 1.0.")
    if not isinstance(image_size, int) or image_size <= 0:
        raise ValueError("image_size must be a positive integer.")
    if not isinstance(max_photons, (int, float)) or max_photons <= 0:
        raise ValueError("max_photons must be positive.")
    if not isinstance(seed, int):
        raise ValueError("seed must be an integer.")
    if not os.path.exists(data_file):
        raise ValueError(f"data_file does not exist: {data_file}")
    np.random.seed(seed)

    # read primary dataset and metadata
    dataset = read_dataset(data_file)
    metadata = read_dataset_metadata(data_file)
    # optionally prepend training data
    if train_data_file is not None:
        train_dataset = read_dataset(train_data_file)
        train_metadata = read_dataset_metadata(train_data_file)
        match, mismatches = _compare_metadata(train_metadata, metadata)
        if not match:
            raise ValueError(f"train_data_file metadata mismatch: {mismatches}")
        # compute sizes
        num_rings_train = sum(len(ev['tracked_rings']) for ev in train_dataset)
        num_rings_valtest = sum(len(ev['tracked_rings']) for ev in dataset)
        tot_num_rings = num_rings_train + num_rings_valtest
        train_size = num_rings_train
        val_size = int(num_rings_valtest * train_val_test_split[1])
        test_size = int(num_rings_valtest * train_val_test_split[2])
        dataset = train_dataset + dataset
    else:
        tot_num_rings = sum(len(ev['tracked_rings']) for ev in dataset)
        train_size = int(tot_num_rings * train_val_test_split[0])
        val_size = int(tot_num_rings * train_val_test_split[1])
        test_size = tot_num_rings - train_size - val_size

    # create directory structure
    os.makedirs(base_dir, exist_ok=True)
    images_dir = os.path.join(base_dir, 'images')
    os.makedirs(images_dir, exist_ok=True)
    # determine sharding thresholds
    if images_per_folder is None:
        images_per_folder = tot_num_rings + 1
    if tot_num_rings > images_per_folder:
        sharding_idxs = np.arange(0, tot_num_rings, images_per_folder)
        # pre-create shard directories
        for idx in range(len(sharding_idxs)):
            os.makedirs(os.path.join(images_dir, f'shard-{idx}'), exist_ok=True)
    else:
        sharding_idxs = None

    # compute normalisation constants
    max_radius = metadata['max_cherenkov_radius'] + 4 * metadata['radial_noise'][1]
    max_distance = max_radius * 2.0  # diameter
    pixel_size = max_distance / image_size
    normalization_factor = (pixel_size ** 2) * max_photons
    scale_factor = (image_size - 1) / max_distance

    ptypes = metadata['particle_types']
    masses = metadata['masses']
    masses_list = list(masses.values())

    all_images: list[str] = []
    all_labels: list[str] = []
    all_momenta: list[float] = []
    saved_images = 0
    shard_idx = 0
    # iterate over events and tracked rings
    for event_idx, event in enumerate(tqdm(dataset, desc="Generating images")):
        centers = event['centers']
        tracked = event['tracked_rings']
        momenta = event['momenta']
        rings = event['rings']
        event_images = []
        event_labels = []
        event_momenta = []
        event_ring_indices = []
        for center_idx in tracked:
            center = centers[center_idx]
            # find rings close to this centre (within 2xmax_radius)
            in_range_idxs = np.where(np.linalg.norm(centers - center, axis=1) <= 2 * max_radius)[0]
            points: list[np.ndarray] = []
            ring_id: list[int] = []
            for ridx in in_range_idxs:
                pts = rings[ridx].copy()
                # shift to local centre
                pts[:, 0] -= center[0]
                pts[:, 1] -= center[1]
                points.append(pts)
                ring_id.extend([ridx] * len(pts))
            if points:
                in_range_rings = np.concatenate(points, axis=0)
                ring_id_arr = np.asarray(ring_id, dtype=np.int32)
            else:
                in_range_rings = np.empty((0, 2), dtype=np.float32)
                ring_id_arr = np.empty((0,), dtype=np.int32)
            # apply annular mask if requested
            if annular_mask and in_range_rings.size > 0:
                distances = np.linalg.norm(in_range_rings, axis=1)
                # compute hypothetical inner and outer radii using min/max particle mass
                min_hyp_ring = calculate_cherenkov_radius(
                    calculate_cherenkov_angle(momenta[center_idx], max(masses_list), metadata['refractive_index']),
                    metadata['refractive_index'],
                    metadata['max_cherenkov_radius']
                ) - 4 * metadata['radial_noise'][1]
                max_hyp_ring = calculate_cherenkov_radius(
                    calculate_cherenkov_angle(momenta[center_idx], min(masses_list), metadata['refractive_index']),
                    metadata['refractive_index'],
                    metadata['max_cherenkov_radius']
                ) + 4 * metadata['radial_noise'][1]
                mask = (distances >= min_hyp_ring) & (distances <= max_hyp_ring)
                in_range_rings = in_range_rings[mask]
                ring_id_arr = ring_id_arr[mask]
            # map to pixel coordinates
            if polar_transform:
                # convert to polar
                r = np.linalg.norm(in_range_rings, axis=1)
                theta = np.arctan2(in_range_rings[:, 1], in_range_rings[:, 0])
                theta = (theta + 2.0 * np.pi) % (2.0 * np.pi)
                theta_scale = (image_size - 1) / (2.0 * np.pi)
                x_pix = np.rint(theta * theta_scale).astype(int)
                if stretch_radii:
                    # compute per-annulus bounds and stretch them to the full image height
                    min_hyp_ring = calculate_cherenkov_radius(
                        calculate_cherenkov_angle(momenta[center_idx], max(masses_list), metadata['refractive_index']),
                        metadata['refractive_index'],
                        metadata['max_cherenkov_radius']
                    ) - 4 * metadata['radial_noise'][1]
                    max_hyp_ring = calculate_cherenkov_radius(
                        calculate_cherenkov_angle(momenta[center_idx], min(masses_list), metadata['refractive_index']),
                        metadata['refractive_index'],
                        metadata['max_cherenkov_radius']
                    ) + 4 * metadata['radial_noise'][1]
                    range_span = max_hyp_ring - min_hyp_ring
                    radius_scale = (image_size - 1) / range_span if range_span > 0 else 1.0
                    # inverted mapping: larger r (outer) -> smaller y (top of image)
                    y_pix = np.rint((max_hyp_ring - r) * radius_scale).astype(int)
                else:
                    # non-stretched: use a fixed global range so all images share the same vertical scale
                    global_max = max_radius
                    radius_scale = (image_size - 1) / global_max if global_max > 0 else 1.0
                    y_pix = np.rint((global_max - r) * radius_scale).astype(int)
            else:
                # Cartesian mapping
                x_pix = np.rint((in_range_rings[:, 0] + max_radius) * scale_factor).astype(int)
                y_pix = np.rint((max_radius - in_range_rings[:, 1]) * scale_factor).astype(int)
            # clip to image boundaries
            mask = (x_pix >= 0) & (x_pix < image_size) & (y_pix >= 0) & (y_pix < image_size)
            x_pix = x_pix[mask]
            y_pix = y_pix[mask]
            ring_id_arr = ring_id_arr[mask]
            # construct image
            if debug_hilight:
                img = np.zeros((image_size, image_size, 3), dtype=np.float32)
                main_mask = (ring_id_arr == center_idx)
                img[y_pix[main_mask], x_pix[main_mask]] = (193/255, 18/255, 31/255)  # highlight colour
                img[y_pix[~main_mask], x_pix[~main_mask]] = (1.0, 1.0, 1.0)
            else:
                img = np.zeros((image_size, image_size), dtype=np.float32)
                for xp, yp in zip(x_pix, y_pix):
                    img[yp, xp] += 1.0
            # normalise
            img = img / normalization_factor
            img = np.clip(img, 0.0, 1.0)
            event_images.append(img)
            event_labels.append(int(event['particle_types'][center_idx]))
            event_momenta.append(float(momenta[center_idx]))
            event_ring_indices.append(int(center_idx))
        # save images and update counts
        for idx, img in enumerate(event_images):
            img_name = f"event-{event_idx}_ridx-{event_ring_indices[idx]}_type-{event_labels[idx]}_mom-{event_momenta[idx]}"
            # determine shard if necessary
            if sharding_idxs is not None and saved_images >= sharding_idxs[min(shard_idx + 1, len(sharding_idxs) - 1)]:
                shard_idx += 1
            subdir = f"shard-{shard_idx}" if sharding_idxs is not None else ""
            if subdir:
                img_path_rel = os.path.join(subdir, img_name)
            else:
                img_path_rel = img_name
            img_path_abs = os.path.join(images_dir, img_path_rel)
            if as_png:
                from PIL import Image
                if debug_normalize and img.max() > img.min():
                    img_norm = (img - img.min()) / (img.max() - img.min())
                else:
                    img_norm = img
                mode = 'RGB' if debug_hilight else 'L'
                Image.fromarray((img_norm * 255).astype(np.uint8), mode=mode).save(img_path_abs + '.png')
                all_images.append(img_path_rel + '.png')
            else:
                # save as .npy
                np.save(img_path_abs + '.npy', img)
                all_images.append(img_path_rel + '.npy')
            all_labels.append(str(event_labels[idx]))
            all_momenta.append(event_momenta[idx])
            saved_images += 1

    # shuffle and split
    combined = list(zip(all_images, all_labels, all_momenta))
    # if external training data is provided we only shuffle val/test
    if train_data_file is not None:
        train_combined = combined[:train_size]
        valtest_combined = combined[train_size:]
        np.random.shuffle(valtest_combined)
        combined = train_combined + valtest_combined
    else:
        np.random.shuffle(combined)
    # write split files
    train_file = os.path.join(base_dir, 'train.txt')
    val_file = os.path.join(base_dir, 'val.txt')
    test_file = os.path.join(base_dir, 'test.txt')
    particles_file = os.path.join(base_dir, 'particles.txt')
    with open(train_file, 'w') as f:
        for img, label, mom in combined[:train_size]:
            f.write(f"{img} {label} {mom}\n")
    with open(val_file, 'w') as f:
        for img, label, mom in combined[train_size:train_size + val_size]:
            f.write(f"{img} {label} {mom}\n")
    with open(test_file, 'w') as f:
        for img, label, mom in combined[train_size + val_size:]:
            f.write(f"{img} {label} {mom}\n")
    # write particles list
    with open(particles_file, 'w') as f:
        for ptype in ptypes:
            f.write(f"{ptype}\n")
    print(f"Generated {len(all_images)} images in {images_dir}")
    print(f"Train set: {train_size} images")
    print(f"Validation set: {val_size} images")
    print(f"Test set: {test_size} images")